{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCAxuF__COni",
        "outputId": "221f17ad-436c-4c41-8561-f74b527d7e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplTKyxcCeUX",
        "outputId": "a7f66b87-2bd9-4158-e1d2-be8fb0050227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/gdrive/MyDrive/cripp_trial /content"
      ],
      "metadata": {
        "id": "U3prga0jCi4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/cripp_trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anUamRNVCkmc",
        "outputId": "2bc2ccbd-8050-4fda-c91f-f458e154b964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cripp_trial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyYAML==5.4"
      ],
      "metadata": {
        "id": "ZMDf793NCm46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.backends.cudnn.enabled  = True\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "VRYESl-kCo2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from configs.config import cfg, merge_cfg_from_file\n",
        "#from datasets.datasets import create_dataset\n",
        "from models.modules import ChangeDetectorDoubleAttDyn, AddSpatialInfo\n",
        "from models.dynamic_speaker import DynamicSpeaker\n",
        "\n",
        "from utils.utils import AverageMeter, accuracy, set_mode, load_checkpoint, \\\n",
        "                        decode_sequence, coco_gen_format_save\n",
        "from utils.vis_utils import visualize_att\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "NMW8BGypCqjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cude\" if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "dkfKMc1UCsPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = '/content/cripp_trial/configs/dynamic/dynamic.yaml'\n",
        "visualize = False\n",
        "snapshot = '/content/cripp_trial/experiments/snapshots/dynamic_checkpoint_9000.pt'\n",
        "gpu=-1"
      ],
      "metadata": {
        "id": "VqXGX1saCuha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "conifgure the cfg file with correct data paths\n",
        "\n",
        "\n",
        "__C.exp_dir = '/content/cripp_trial/inference'\n",
        "__C.exp_name = 'dynamic'\n",
        "\n",
        "__C.data.dataset = 'rcc_dataset'\n",
        "__C.data.num_workers = 8\n",
        "__C.data.default_feature_dir = '/content/cripp_trial/features'\n",
        "__C.data.semantic_feature_dir = '/content/cripp_trial/data/sc_features'\n",
        "__C.data.nonsemantic_feature_dir = '/content/cripp_trial/data/nsc_features'\n",
        "__C.data.default_img_dir = '/content/cripp_trial/images'\n",
        "__C.data.semantic_img_dir = '/content/cripp_trial/data/sc_images'\n",
        "__C.data.nonsemantic_img_dir = '/content/cripp_trial/data/nsc_images'\n",
        "__C.data.vocab_json = '/content/cripp_trial/jsons/vocab.json'\n",
        "__C.data.splits_json = '/content/cripp_trial/jsons/splits.json'\n",
        "__C.data.h5_label_file = '/content/cripp_trial/jsons/labels.h5'\n",
        "__C.data.h5_ref_label_file = '/content/cripp_trial/jsons/labels.h5'\n",
        "__C.data.type_mapping_json = '/content/cripp_trial/jsons/type_mapping.json'\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "FZWCNeksCwNj",
        "outputId": "668468d6-ffa3-4f48-a556-2298ca575430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nconifgure the cfg file with correct data paths\\n\\n\\n__C.exp_dir = '/content/cripp_trial/inference'\\n__C.exp_name = 'dynamic'\\n\\n__C.data.dataset = 'rcc_dataset'\\n__C.data.num_workers = 8\\n__C.data.default_feature_dir = '/content/cripp_trial/features'\\n__C.data.semantic_feature_dir = '/content/cripp_trial/data/sc_features'\\n__C.data.nonsemantic_feature_dir = '/content/cripp_trial/data/nsc_features'\\n__C.data.default_img_dir = '/content/cripp_trial/images'\\n__C.data.semantic_img_dir = '/content/cripp_trial/data/sc_images'\\n__C.data.nonsemantic_img_dir = '/content/cripp_trial/data/nsc_images'\\n__C.data.vocab_json = '/content/cripp_trial/jsons/vocab.json'\\n__C.data.splits_json = '/content/cripp_trial/jsons/splits.json'\\n__C.data.h5_label_file = '/content/cripp_trial/jsons/labels.h5'\\n__C.data.h5_ref_label_file = '/content/cripp_trial/jsons/labels.h5'\\n__C.data.type_mapping_json = '/content/cripp_trial/jsons/type_mapping.json'\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "in dynamic.yml changes\n",
        "\n",
        "input_dim: 2048 + 4\n",
        "input_dim: 1024 + 2\n",
        "\n",
        "\n",
        "vocab_json: '/content/cripp_trial/jsons/vocab.json'\n",
        "    h5_label_file: '/content/cripp_trial/jsons/labels.h5'\n",
        "    type_mapping_json: '/content/cripp_trial/jsons/type_mapping.json'\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Dk2GLBuCCw8e",
        "outputId": "8672f6dc-4615-45e6-c8e5-aff670b7a9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nin dynamic.yml changes\\n\\ninput_dim: 2048 + 4\\ninput_dim: 1024 + 2\\n\\n\\nvocab_json: '/content/cripp_trial/jsons/vocab.json'\\n    h5_label_file: '/content/cripp_trial/jsons/labels.h5'\\n    type_mapping_json: '/content/cripp_trial/jsons/type_mapping.json'\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_cfg_from_file(cfg)"
      ],
      "metadata": {
        "id": "YVfEaPB-C0Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an inference directory\n",
        "exp_dir = '/content/cripp_trial/inference2'\n",
        "exp_name = 'dynamic'\n",
        "output_dir = os.path.join(exp_dir, exp_name)\n",
        "test_output_dir = os.path.join(output_dir, 'test_output')\n",
        "if not os.path.exists(test_output_dir):\n",
        "    os.makedirs(test_output_dir)\n",
        "caption_output_path = os.path.join(test_output_dir, 'captions', 'test')\n",
        "if not os.path.exists(caption_output_path):\n",
        "    os.makedirs(caption_output_path)\n",
        "att_output_path = os.path.join(test_output_dir, 'attentions', 'test')\n",
        "if not os.path.exists(att_output_path):\n",
        "    os.makedirs(att_output_path)"
      ],
      "metadata": {
        "id": "CpzNg_60C2FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#snapshot_dir = os.path.join(output_dir, 'snapshots')\n",
        "#snapshot_file = '%s_checkpoint_%d.pt' % (exp_name, snapshot)\n",
        "snapshot_full_path = '/content/cripp_trial/experiments/snapshots/dynamic_checkpoint_9000.pt'\n",
        "checkpoint = load_checkpoint(snapshot_full_path)\n",
        "change_detector_state = checkpoint['change_detector_state']\n",
        "speaker_state = checkpoint['speaker_state']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f3KLpQ3C3-N",
        "outputId": "7616e60a-78c6-4f3f-a7b7-d4c6e07b0341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint from /content/cripp_trial/experiments/snapshots/dynamic_checkpoint_9000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modules.py File"
      ],
      "metadata": {
        "id": "AEyNfVJjC5sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ChangeDetectorDoubleAttDyn(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_dim = 2052\n",
        "        self.dim = 128\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv2d(self.input_dim, self.dim, kernel_size=1, padding=0),\n",
        "            nn.GroupNorm(32, self.dim),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.att = nn.Conv2d(self.dim, 1, kernel_size=1, padding=0)\n",
        "        self.fc1 = nn.Linear(self.input_dim // 2, 6)\n",
        "\n",
        "    def forward(self, input_1, input_2):\n",
        "        batch_size, _, H, W = input_1.size()\n",
        "        input_diff = input_2 - input_1\n",
        "        input_before = torch.cat([input_1, input_diff], 1)\n",
        "        input_after = torch.cat([input_2, input_diff], 1)\n",
        "        embed_before = self.embed(input_before)\n",
        "        embed_after = self.embed(input_after)\n",
        "        att_weight_before = F.sigmoid(self.att(embed_before))\n",
        "        att_weight_after = F.sigmoid(self.att(embed_after))\n",
        "\n",
        "        att_1_expand = att_weight_before.expand_as(input_1)\n",
        "        attended_1 = (input_1 * att_1_expand).sum(2).sum(2)  # (batch, dim)\n",
        "        att_2_expand = att_weight_after.expand_as(input_2)\n",
        "        attended_2 = (input_2 * att_2_expand).sum(2).sum(2)  # (batch, dim)\n",
        "        input_attended = attended_2 - attended_1\n",
        "        pred = self.fc1(input_attended)\n",
        "\n",
        "        return pred, att_weight_before, att_weight_after, attended_1, attended_2, input_attended\n",
        "\n",
        "\n",
        "class AddSpatialInfo(nn.Module):\n",
        "\n",
        "    def _create_coord(self, img_feat):\n",
        "        batch_size, _, h, w = img_feat.size()\n",
        "        coord_map = img_feat.new_zeros(2, h, w)\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                coord_map[0][i][j] = (j * 2.0 / w) - 1\n",
        "                coord_map[1][i][j] = (i * 2.0 / h) - 1\n",
        "        sequence = [coord_map] * batch_size\n",
        "        coord_map_in_batch = torch.stack(sequence)\n",
        "        return coord_map_in_batch\n",
        "\n",
        "    def forward(self, img_feat):\n",
        "        coord_map = self._create_coord(img_feat)\n",
        "        img_feat_aug = torch.cat([img_feat, coord_map], dim=1)\n",
        "        return img_feat_aug\n"
      ],
      "metadata": {
        "id": "S2LLk7zjC8TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CaptionModel.py File"
      ],
      "metadata": {
        "id": "APfeyoXODSAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This file contains ShowAttendTell and AllImg model\n",
        "\n",
        "# ShowAttendTell is from Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\n",
        "# https://arxiv.org/abs/1502.03044\n",
        "\n",
        "# AllImg is a model where\n",
        "# img feature is concatenated with word embedding at every time step as the input of lstm\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import *\n",
        "\n",
        "\n",
        "class CaptionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CaptionModel, self).__init__()\n",
        "\n",
        "    # implements beam search\n",
        "    # calls beam_step and returns the final set of beams\n",
        "    # augments log-probabilities with diversity terms when number of groups > 1\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        mode = kwargs.get('mode', 'forward')\n",
        "        if 'mode' in kwargs:\n",
        "            del kwargs['mode']\n",
        "        return getattr(self, '_'+mode)(*args, **kwargs)\n",
        "\n",
        "    def beam_search(self, init_state, init_logprobs, *args, **kwargs):\n",
        "\n",
        "        # function computes the similarity score to be augmented\n",
        "        def add_diversity(beam_seq_table, logprobsf, t, divm, diversity_lambda, bdash):\n",
        "            local_time = t - divm\n",
        "            unaug_logprobsf = logprobsf.clone()\n",
        "            for prev_choice in range(divm):\n",
        "                prev_decisions = beam_seq_table[prev_choice][local_time]\n",
        "                for sub_beam in range(bdash):\n",
        "                    for prev_labels in range(bdash):\n",
        "                        logprobsf[sub_beam][prev_decisions[prev_labels]] = logprobsf[sub_beam][prev_decisions[prev_labels]] - diversity_lambda\n",
        "            return unaug_logprobsf\n",
        "\n",
        "        # does one step of classical beam search\n",
        "\n",
        "        def beam_step(logprobsf, unaug_logprobsf, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n",
        "            #INPUTS:\n",
        "            #logprobsf: probabilities augmented after diversity\n",
        "            #beam_size: obvious\n",
        "            #t        : time instant\n",
        "            #beam_seq : tensor contanining the beams\n",
        "            #beam_seq_logprobs: tensor contanining the beam logprobs\n",
        "            #beam_logprobs_sum: tensor contanining joint logprobs\n",
        "            #OUPUTS:\n",
        "            #beam_seq : tensor containing the word indices of the decoded captions\n",
        "            #beam_seq_logprobs : log-probability of each decision made, same size as beam_seq\n",
        "            #beam_logprobs_sum : joint log-probability of each beam\n",
        "\n",
        "            ys,ix = torch.sort(logprobsf,1,True)\n",
        "            candidates = []\n",
        "            cols = min(beam_size, ys.size(1))\n",
        "            rows = beam_size\n",
        "            if t == 0:\n",
        "                rows = 1\n",
        "            for c in range(cols): # for each column (word, essentially)\n",
        "                for q in range(rows): # for each beam expansion\n",
        "                    #compute logprob of expanding beam q with word in (sorted) position c\n",
        "                    local_logprob = ys[q,c].item()\n",
        "                    candidate_logprob = beam_logprobs_sum[q] + local_logprob\n",
        "                    local_unaug_logprob = unaug_logprobsf[q,ix[q,c]]\n",
        "                    candidates.append({'c':ix[q,c], 'q':q, 'p':candidate_logprob, 'r':local_unaug_logprob})\n",
        "            candidates = sorted(candidates,  key=lambda x: -x['p'])\n",
        "            \n",
        "            new_state = [_.clone() for _ in state]\n",
        "            #beam_seq_prev, beam_seq_logprobs_prev\n",
        "            if t >= 1:\n",
        "            #we''ll need these as reference when we fork beams around\n",
        "                beam_seq_prev = beam_seq[:t].clone()\n",
        "                beam_seq_logprobs_prev = beam_seq_logprobs[:t].clone()\n",
        "            for vix in range(beam_size):\n",
        "                v = candidates[vix]\n",
        "                #fork beam index q into index vix\n",
        "                if t >= 1:\n",
        "                    beam_seq[:t, vix] = beam_seq_prev[:, v['q']]\n",
        "                    beam_seq_logprobs[:t, vix] = beam_seq_logprobs_prev[:, v['q']]\n",
        "                #rearrange recurrent states\n",
        "                for state_ix in range(len(new_state)):\n",
        "                #  copy over state in previous beam q to new beam at vix\n",
        "                    new_state[state_ix][:, vix] = state[state_ix][:, v['q']] # dimension one is time step\n",
        "                #append new end terminal at the end of this beam\n",
        "                beam_seq[t, vix] = v['c'] # c'th word is the continuation\n",
        "                beam_seq_logprobs[t, vix] = v['r'] # the raw logprob here\n",
        "                beam_logprobs_sum[vix] = v['p'] # the new (sum) logprob along this beam\n",
        "            state = new_state\n",
        "            return beam_seq,beam_seq_logprobs,beam_logprobs_sum,state,candidates\n",
        "\n",
        "        # Start diverse_beam_search\n",
        "        beam_size = 10\n",
        "        group_size = 1\n",
        "        diversity_lambda = 0.5\n",
        "        decoding_constraint = 0\n",
        "        max_ppl = 0\n",
        "        bdash = beam_size // group_size # beam per group\n",
        "\n",
        "        # INITIALIZATIONS\n",
        "        beam_seq_table = [torch.LongTensor(self.seq_length, bdash).zero_() for _ in range(group_size)]\n",
        "        beam_seq_logprobs_table = [torch.FloatTensor(self.seq_length, bdash).zero_() for _ in range(group_size)]\n",
        "        beam_logprobs_sum_table = [torch.zeros(bdash) for _ in range(group_size)]\n",
        "\n",
        "        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size)\n",
        "        done_beams_table = [[] for _ in range(group_size)]\n",
        "        state_table = [list(torch.unbind(_)) for _ in torch.stack(init_state).chunk(group_size, 2)]\n",
        "        logprobs_table = list(init_logprobs.chunk(group_size, 0))\n",
        "        # END INIT\n",
        "\n",
        "        # Chunk elements in the args\n",
        "        args = list(args)\n",
        "        args = [_.chunk(group_size) if _ is not None else [None]*group_size for _ in args]\n",
        "        args = [[args[i][j] for i in range(len(args))] for j in range(group_size)]\n",
        "\n",
        "        for t in range(self.seq_length + group_size - 1):\n",
        "            for divm in range(group_size): \n",
        "                if t >= divm and t <= self.seq_length + divm - 1:\n",
        "                    # add diversity\n",
        "                    logprobsf = logprobs_table[divm].data.float()\n",
        "                    # suppress previous word\n",
        "                    if decoding_constraint and t-divm > 0:\n",
        "                        logprobsf.scatter_(1, beam_seq_table[divm][t-divm-1].unsqueeze(1).to(device), float('-inf'))\n",
        "                    # suppress UNK tokens in the decoding (here <UNK> has an index of 1)\n",
        "                    logprobsf[:, 1] = logprobsf[:, 1] - 1000  \n",
        "                    # diversity is added here\n",
        "                    # the function directly modifies the logprobsf values and hence, we need to return\n",
        "                    # the unaugmented ones for sorting the candidates in the end. # for historical\n",
        "                    # reasons :-)\n",
        "                    unaug_logprobsf = add_diversity(beam_seq_table,logprobsf,t,divm,diversity_lambda,bdash)\n",
        "\n",
        "                    # infer new beams\n",
        "                    beam_seq_table[divm],\\\n",
        "                    beam_seq_logprobs_table[divm],\\\n",
        "                    beam_logprobs_sum_table[divm],\\\n",
        "                    state_table[divm],\\\n",
        "                    candidates_divm = beam_step(logprobsf,\n",
        "                                                unaug_logprobsf,\n",
        "                                                bdash,\n",
        "                                                t-divm,\n",
        "                                                beam_seq_table[divm],\n",
        "                                                beam_seq_logprobs_table[divm],\n",
        "                                                beam_logprobs_sum_table[divm],\n",
        "                                                state_table[divm])\n",
        "\n",
        "                    # if time's up... or if end token is reached then copy beams\n",
        "                    for vix in range(bdash):\n",
        "                        if beam_seq_table[divm][t-divm,vix] == 0 or t == self.seq_length + divm - 1:\n",
        "                            final_beam = {\n",
        "                                'seq': beam_seq_table[divm][:, vix].clone(), \n",
        "                                'logps': beam_seq_logprobs_table[divm][:, vix].clone(),\n",
        "                                'unaug_p': beam_seq_logprobs_table[divm][:, vix].sum().item(),\n",
        "                                'p': beam_logprobs_sum_table[divm][vix].item()\n",
        "                            }\n",
        "                            if max_ppl:\n",
        "                                final_beam['p'] = final_beam['p'] / (t-divm+1)\n",
        "                            done_beams_table[divm].append(final_beam)\n",
        "                            # don't continue beams from finished sequences\n",
        "                            beam_logprobs_sum_table[divm][vix] = -1000\n",
        "\n",
        "                    # move the current group one step forward in time\n",
        "                    \n",
        "                    it = beam_seq_table[divm][t-divm]\n",
        "                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(it.to(device), *(args[divm] + [state_table[divm]]))\n",
        "\n",
        "        # all beams are sorted by their log-probabilities\n",
        "        done_beams_table = [sorted(done_beams_table[i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n",
        "        done_beams = reduce(lambda a,b:a+b, done_beams_table)\n",
        "        return done_beams\n"
      ],
      "metadata": {
        "id": "NOekVkECDUp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DynamicSpeaker.py File"
      ],
      "metadata": {
        "id": "HMnMnLx5DlGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from utils.attr_dict import AttrDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "speaker_rnn_size = 512\n",
        "speaker_input_dim = 1026\n",
        "speaker_word_embed_size = 300\n",
        "model_speaker = AttrDict()\n",
        "drop_prob_lm = 0.5\n",
        "embed_input_dim = 3078\n",
        "embed_dim = 512\n",
        "\n",
        "def sort_pack_padded_sequence(input, lengths):\n",
        "    sorted_lengths, indices = torch.sort(lengths, descending=True)\n",
        "    tmp = pack_padded_sequence(input[indices], sorted_lengths, batch_first=True)\n",
        "    inv_ix = indices.clone()\n",
        "    inv_ix[indices] = inv_ix.new_tensor(torch.arange(0, len(indices)))\n",
        "    return tmp, inv_ix\n",
        "\n",
        "\n",
        "def pad_unsort_packed_sequence(input, inv_ix):\n",
        "    tmp, _ = pad_packed_sequence(input, batch_first=True)\n",
        "    tmp = tmp[inv_ix]\n",
        "    return tmp\n",
        "\n",
        "\n",
        "def last_timestep(unpacked, lengths):\n",
        "    # Index of the last output for each sequence.\n",
        "    idx = (lengths - 1).view(-1, 1).expand(unpacked.size(0),\n",
        "                                           unpacked.size(2)).unsqueeze(1)\n",
        "    return unpacked.gather(1, idx).squeeze()\n",
        "\n",
        "\n",
        "def pack_wrapper(module, att_feats, att_masks):\n",
        "    if att_masks is not None:\n",
        "        packed, inv_ix = sort_pack_padded_sequence(att_feats, att_masks.data.long().sum(1))\n",
        "        return pad_unsort_packed_sequence(PackedSequence(module(packed[0]), packed[1]), inv_ix)\n",
        "    else:\n",
        "        return module(att_feats)\n",
        "\n",
        "\n",
        "class DynamicCore(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn_num_layers = 2\n",
        "        self.drop_prob_lm = 0.5\n",
        "        self.input_dim = speaker_input_dim\n",
        "        self.embed_input_dim = 3078\n",
        "        self.embed_dim = 512\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(self.embed_input_dim, self.embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.drop_prob_lm)\n",
        "        )\n",
        "\n",
        "        self.module_att_lstm = nn.LSTMCell(self.embed_dim + speaker_rnn_size,\n",
        "                                           speaker_rnn_size)\n",
        "\n",
        "        self.weight_fc = nn.Sequential(\n",
        "            nn.Linear(speaker_rnn_size, 3),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.lang_lstm = nn.LSTMCell(speaker_word_embed_size + \\\n",
        "                                     speaker_input_dim,\n",
        "                                     speaker_rnn_size)\n",
        "        self.module_weights = None\n",
        "\n",
        "\n",
        "    def forward(self, xt,\n",
        "                loc_feat_bef,loc_feat_aft,\n",
        "                feat_diff, state):\n",
        "\n",
        "        prev_h = state[0][-1]  # prev hidden state from the lang_lstm\n",
        "        embed_input = torch.cat([loc_feat_bef, feat_diff, loc_feat_aft], 1)\n",
        "        embed = self.embed(embed_input)\n",
        "        module_att_lstm_input = torch.cat([embed, prev_h], 1)\n",
        "\n",
        "        h_mod_att, c_mod_att = self.module_att_lstm(module_att_lstm_input, (state[0][0], state[1][0]))\n",
        "        module_weights = self.weight_fc(h_mod_att)\n",
        "        self.module_weights = module_weights\n",
        "\n",
        "        feats = torch.cat([loc_feat_bef.unsqueeze(1),\n",
        "                           feat_diff.unsqueeze(1),\n",
        "                           loc_feat_aft.unsqueeze(1)], 1)\n",
        "\n",
        "        weights_expand = module_weights.unsqueeze(2).expand_as(feats)\n",
        "        att_feat = (feats * weights_expand).sum(1)  # (batch, feat_dim)\n",
        "\n",
        "        lang_lstm_input = torch.cat([xt, att_feat], 1)\n",
        "        h_lang, c_lang = self.lang_lstm(lang_lstm_input, (state[0][1], state[1][1]))\n",
        "\n",
        "        output = F.dropout(h_lang, self.drop_prob_lm, self.training)\n",
        "        state = (torch.stack([h_mod_att, h_lang]), torch.stack([c_mod_att, c_lang]))\n",
        "\n",
        "        return output, state\n",
        "\n",
        "\n",
        "    def get_module_weights(self):\n",
        "        # needs to be called after forward call\n",
        "        return self.module_weights\n",
        "\n",
        "\n",
        "class DynamicSpeaker(CaptionModel):\n",
        "    def __init__(self):\n",
        "        super(DynamicSpeaker, self).__init__()\n",
        "        self.vocab_size = 76\n",
        "        self.word_embed_size = speaker_word_embed_size\n",
        "        self.rnn_size = speaker_rnn_size\n",
        "        self.drop_prob_lm = 0.5\n",
        "        self.seq_length = 23\n",
        "\n",
        "        self.ss_prob = 0.0  # Scheduled sampling probability\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Embedding(self.vocab_size, self.word_embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.drop_prob_lm))\n",
        "\n",
        "        self.core = DynamicCore()\n",
        "        self.rnn_num_layers = self.core.rnn_num_layers\n",
        "\n",
        "        self.logit_layers = getattr(model_speaker, 'logit_layers', 1)\n",
        "        if self.logit_layers == 1:\n",
        "            self.logit = nn.Linear(self.rnn_size, self.vocab_size)\n",
        "        else:\n",
        "            self.logit = [[nn.Linear(self.rnn_size, self.rnn_size),\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(0.5)] \\\n",
        "                          for _ in range(model_speaker.logit_layers - 1)]\n",
        "            self.logit = nn.Sequential(*(\n",
        "                    reduce(lambda x, y: x + y, self.logit) + \\\n",
        "                    [nn.Linear(self.rnn_size, self.vocab_size)]))\n",
        "\n",
        "        self.module_weights = []\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.rnn_num_layers, batch_size, self.rnn_size),\n",
        "                weight.new_zeros(self.rnn_num_layers, batch_size, self.rnn_size))\n",
        "\n",
        "\n",
        "    def _forward(self,\n",
        "                 feat_bef, feat_aft,\n",
        "                 feat_diff, seq):\n",
        "\n",
        "        # start fresh\n",
        "        self.module_weights = []\n",
        "\n",
        "        batch_size = feat_bef.size(0)\n",
        "        state = self.init_hidden(batch_size)\n",
        "\n",
        "        # outputs are logprobs\n",
        "        outputs = feat_bef.new_zeros(batch_size, self.seq_length, self.vocab_size)\n",
        "\n",
        "        for i in range(self.seq_length):\n",
        "            if self.training and i >= 1 and self.ss_prob > 0.0:\n",
        "                sample_prob = feat_bef.new(batch_size).uniform_(0, 1)\n",
        "                sample_mask = sample_prob < self.ss_prob\n",
        "                if sample_mask.sum() == 0:\n",
        "                    it = seq[:, i].clone()\n",
        "                else:\n",
        "                    sample_ind = sample_mask.nonzero().view(-1)  # idx for sampling from batch\n",
        "                    it = seq[:, i].data.clone()\n",
        "                    prob_prev = torch.exp(outputs[:, i - 1].detach())  # fetch previous distribution (N x (M+1))\n",
        "                    it.index_copy_(0, sample_ind,\n",
        "                                   torch.multinomial(prob_prev, 1).view(-1).index_select(0, sample_ind))\n",
        "            else:\n",
        "                it = seq[:, i].clone()\n",
        "            # break if all the sequences end\n",
        "            if i >= 1 and seq[:, i].sum() == 0:\n",
        "                break\n",
        "\n",
        "            output, state = self.get_logprobs_state(it,\n",
        "                                                    feat_bef, feat_aft,\n",
        "                                                    feat_diff, state)\n",
        "            outputs[:, i] = output\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def get_logprobs_state(self, it,\n",
        "                           feat_bef, feat_aft,\n",
        "                           feat_diff, state):\n",
        "\n",
        "        # 'it' contains word indices\n",
        "        xt = self.embed(it)\n",
        "\n",
        "        output, state = self.core(xt,\n",
        "                                  feat_bef, feat_aft,\n",
        "                                  feat_diff, state)\n",
        "\n",
        "        # after every call of the core, collect the module weights\n",
        "        self.module_weights.append(self.core.get_module_weights())\n",
        "        log_probs = F.log_softmax(self.logit(output), dim=1)\n",
        "        return log_probs, state\n",
        "\n",
        "\n",
        "    def get_module_weights(self):\n",
        "        if len(self.module_weights) == 0:\n",
        "            print('no module weights accumulated')\n",
        "            return None\n",
        "        module_weights_stacked = torch.stack(self.module_weights, dim=1)\n",
        "        return module_weights_stacked\n",
        "\n",
        "\n",
        "    def _sample_beam(self,\n",
        "                     feat_bef, feat_aft,\n",
        "                     feat_diff):\n",
        "        # start fresh\n",
        "        self.module_weights = []\n",
        "\n",
        "        beam_size = 10\n",
        "        batch_size = feat_bef.size(0)\n",
        "\n",
        "        assert beam_size <= self.vocab_size, 'lets assume this for now, otherwise this corner case causes a few headaches down the road. can be dealt with in future if needed'\n",
        "        seq = torch.LongTensor(self.seq_length, batch_size).zero_()\n",
        "        seq_logprobs = torch.FloatTensor(self.seq_length, batch_size)\n",
        "\n",
        "        self.done_beams = [[] for _ in range(batch_size)]\n",
        "        for k in range(batch_size):\n",
        "            state = self.init_hidden(beam_size)\n",
        "            tmp_feat_bef = feat_bef[k:k + 1].expand(beam_size, -1).contiguous()\n",
        "            tmp_feat_aft = feat_aft[k:k + 1].expand(beam_size, -1).contiguous()\n",
        "            tmp_feat_diff = feat_diff[k:k + 1].expand(beam_size, -1).contiguous()\n",
        "\n",
        "            # input <bos> (idx of 2)\n",
        "            it = feat_bef.new_zeros([beam_size], dtype=torch.long) + 2\n",
        "            logprobs, state = self.get_logprobs_state(it,\n",
        "                                                      tmp_feat_bef, tmp_feat_aft,\n",
        "                                                      tmp_feat_diff, state)\n",
        "\n",
        "            self.done_beams[k] = self.beam_search(state, logprobs,\n",
        "                                                  tmp_feat_bef, tmp_feat_aft,\n",
        "                                                  tmp_feat_diff, state)\n",
        "            seq[:, k] = self.done_beams[k][0]['seq']\n",
        "            seq_logprobs[:, k] = self.done_beams[k][0]['logps']\n",
        "\n",
        "        # return the samples and their log likelihoods\n",
        "        return seq.transpose(0, 1), seq_logprobs.transpose(0, 1)\n",
        "\n",
        "\n",
        "    def _sample(self,\n",
        "                feat_bef, feat_aft,\n",
        "                feat_diff, seq, sample_max=0):\n",
        "\n",
        "        # start fresh\n",
        "        self.module_weights = []\n",
        "\n",
        "        #sample_max = cfg.model.speaker.get('sample_max', 1)\n",
        "        beam_size = 1\n",
        "        temperature = 1.0\n",
        "        decoding_constraint = 0\n",
        "\n",
        "        if beam_size > 1:\n",
        "            return self._sample_beam(feat_bef, feat_aft,\n",
        "                                     feat_diff)\n",
        "\n",
        "        batch_size = feat_bef.size(0)\n",
        "        state = self.init_hidden(batch_size)\n",
        "\n",
        "        seq = feat_bef.new_zeros((batch_size, self.seq_length), dtype=torch.long)\n",
        "        seq_logprobs = feat_bef.new_zeros(batch_size, self.seq_length)\n",
        "\n",
        "        for t in range(self.seq_length + 1):\n",
        "            if t == 0:  # input <bos>\n",
        "                it = feat_bef.new_zeros(batch_size, dtype=torch.long) + 2\n",
        "\n",
        "            logprobs, state = self.get_logprobs_state(it,\n",
        "                                                      feat_bef, feat_aft,\n",
        "                                                      feat_diff, state)\n",
        "\n",
        "            # if first step, make sure we don't sample NULL\n",
        "            if t == 0:\n",
        "                tmp = logprobs.new_zeros(logprobs.size())\n",
        "                tmp[:, 0] = float('-inf')\n",
        "                logprobs = logprobs + tmp\n",
        "            # decoding constraint for not sampling the word sampled at t-1\n",
        "            if decoding_constraint and t > 0:\n",
        "                tmp = logprobs.new_zeros(logprobs.size())\n",
        "                tmp.scatter_(1, seq[:, t - 1].data.unsqueeze(1), float('-inf'))\n",
        "                logprobs = logprobs + tmp\n",
        "\n",
        "            # sample the next word\n",
        "            if t == self.seq_length:  # skip if we achieve maximum length\n",
        "                break\n",
        "            if sample_max:\n",
        "                sample_logprobs, it = torch.max(logprobs.data, 1)\n",
        "                it = it.view(-1).long()\n",
        "            else:\n",
        "                if temperature == 1.0:\n",
        "                    prob_prev = torch.exp(logprobs.data)\n",
        "                else:\n",
        "                    prob_prev = torch.exp(torch.div(logprobs.data, temperature))\n",
        "                it = torch.multinomial(prob_prev, 1)\n",
        "                sample_logprobs = logprobs.gather(1, it)  # gather the logprobs at sampled positions\n",
        "                it = it.view(-1).long()  # and flatten indices for downstream processing\n",
        "\n",
        "            # stop when all finished\n",
        "            if t == 0:\n",
        "                unfinished = it > 0\n",
        "            else:\n",
        "                unfinished = unfinished * (it > 0)\n",
        "            it = it * unfinished.type_as(it)\n",
        "            seq[:, t] = it\n",
        "            seq_logprobs[:, t] = sample_logprobs.view(-1)\n",
        "            # quit loop if all sequences have finished\n",
        "            if unfinished.sum() == 0:\n",
        "                break\n",
        "\n",
        "        return seq, seq_logprobs\n",
        "\n"
      ],
      "metadata": {
        "id": "0SuMqxGSDoS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instanciating the Module classes"
      ],
      "metadata": {
        "id": "piqywm_qVNpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load modules\n",
        "change_detector = ChangeDetectorDoubleAttDyn()\n",
        "change_detector.load_state_dict(change_detector_state)\n",
        "change_detector = change_detector.to(device)\n",
        "\n",
        "speaker = DynamicSpeaker()\n",
        "speaker.load_state_dict(speaker_state)\n",
        "speaker.to(device)\n",
        "\n",
        "spatial_info = AddSpatialInfo()\n",
        "spatial_info.to(device)\n",
        "\n",
        "print(change_detector)\n",
        "print(speaker)\n",
        "print(spatial_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj3sfvd_EEPn",
        "outputId": "7668f386-c4e6-47e1-b428-9dbfca4b48a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChangeDetectorDoubleAttDyn(\n",
            "  (embed): Sequential(\n",
            "    (0): Conv2d(2052, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (att): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=1026, out_features=6, bias=True)\n",
            ")\n",
            "DynamicSpeaker(\n",
            "  (embed): Sequential(\n",
            "    (0): Embedding(76, 300)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (core): DynamicCore(\n",
            "    (embed): Sequential(\n",
            "      (0): Linear(in_features=3078, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (module_att_lstm): LSTMCell(1024, 512)\n",
            "    (weight_fc): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=3, bias=True)\n",
            "      (1): Softmax(dim=1)\n",
            "    )\n",
            "    (lang_lstm): LSTMCell(1326, 512)\n",
            "  )\n",
            "  (logit): Linear(in_features=512, out_features=76, bias=True)\n",
            ")\n",
            "AddSpatialInfo()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Renaming SC_features and SC_images"
      ],
      "metadata": {
        "id": "FwWbHpZjU458"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/content/cripp_trial/data/sc_images'\n",
        "files = os.listdir(path)\n",
        "\n",
        "\n",
        "for index, file in enumerate(files):\n",
        "  if (index<10):\n",
        "    os.rename(os.path.join(path, file), os.path.join(path, ''.join([\"img_000\", str(index), '.png'])))\n",
        "  else:\n",
        "    os.rename(os.path.join(path, file), os.path.join(path, ''.join([\"img_00\", str(index), '.png'])))"
      ],
      "metadata": {
        "id": "CvB2GD8lU3YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/cripp_trial/data/sc_features'\n",
        "#path='/content/cripp_trial/data/resnet34/sc_features'\n",
        "files = os.listdir(path)\n",
        "\n",
        "\n",
        "for index, file in enumerate(files):\n",
        "  if (index<10):\n",
        "    os.rename(os.path.join(path, file), os.path.join(path, ''.join([\"img_000\", str(index), '.png.npy'])))\n",
        "  else:\n",
        "    os.rename(os.path.join(path, file), os.path.join(path, ''.join([\"img_00\", str(index), '.png.npy'])))"
      ],
      "metadata": {
        "id": "04CWAZoMWxUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RCCDataset.py File"
      ],
      "metadata": {
        "id": "iKwoc4toOCxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "vocab_json='/content/cripp_trial/jsons/vocab.json'\n",
        "type_mapping_json = '/content/cripp_trial/jsons/type_mapping.json'\n",
        "default_feature_dir = '/content/cripp_trial/data/features'\n",
        "semantic_feature_dir = '/content/cripp_trial/data/sc_features'\n",
        "nonsemantic_feature_dir = '/content/cripp_trial/data/nsc_features'\n",
        "default_img_dir = '/content/cripp_trial/data/images'\n",
        "semantic_img_dir = '/content/cripp_trial/data/sc_images'\n",
        "nonsemantic_img_dir = '/content/cripp_trial/data/nsc_images'\n",
        "splits_json = '/content/cripp_trial/splits.json'\n",
        "train_batch_size = 128\n",
        "max_samples = None\n",
        "val_batch_size = 64\n",
        "test_batch_size = 1\n",
        "h5_label_file = '/content/cripp_trial/jsons/labels.h5'\n",
        "\n",
        "\n",
        "\n",
        "class RCCDataset(Dataset):\n",
        "\n",
        "    shapes = set(['ball', 'block', 'cube', 'cylinder', 'sphere'])\n",
        "    sphere = set(['ball', 'sphere'])\n",
        "    cube = set(['block', 'cube'])\n",
        "    cylinder = set(['cylinder'])\n",
        "\n",
        "    colors = set(['red', 'cyan', 'brown', 'blue', 'purple', 'green', 'gray', 'yellow'])\n",
        "\n",
        "    materials = set(['metallic', 'matte', 'rubber', 'shiny', 'metal'])\n",
        "    rubber = set(['matte', 'rubber'])\n",
        "    metal = set(['metal', 'metallic', 'shiny'])\n",
        "\n",
        "    type_to_label = {\n",
        "        'color': 0,\n",
        "        'material': 1,\n",
        "        'add': 2,\n",
        "        'drop': 3,\n",
        "        'move': 4,\n",
        "        'no_change': 5\n",
        "    }\n",
        "\n",
        "    def __init__(self, split):\n",
        "\n",
        "        print('Speaker Dataset loading vocab json file: ', vocab_json)\n",
        "        self.vocab_json = vocab_json\n",
        "        self.word_to_idx = json.load(open(self.vocab_json, 'r'))\n",
        "        self.idx_to_word = {}\n",
        "        for word, idx in self.word_to_idx.items():\n",
        "            self.idx_to_word[idx] = word\n",
        "        self.vocab_size = len(self.idx_to_word)\n",
        "        print('vocab size is ', self.vocab_size)\n",
        "\n",
        "        self.type_mapping = json.load(open(type_mapping_json, 'r'))\n",
        "        self.type_to_img = {}\n",
        "        for k, v in self.type_mapping.items():\n",
        "            self.type_to_img[k] = set([int(x.split('.')[0]) for x in v])\n",
        "\n",
        "\n",
        "        self.d_feat_dir = default_feature_dir\n",
        "        self.s_feat_dir = semantic_feature_dir\n",
        "        self.n_feat_dir = nonsemantic_feature_dir\n",
        "\n",
        "        self.d_feats = sorted(os.listdir(self.d_feat_dir))\n",
        "        self.s_feats = sorted(os.listdir(self.s_feat_dir))\n",
        "        self.n_feats = sorted(os.listdir(self.n_feat_dir))\n",
        "\n",
        "        assert len(self.d_feats) == len(self.s_feats) == len(self.n_feats), \\\n",
        "            'The number of features are different from each other!'\n",
        "        \n",
        "        self.d_img_dir = default_img_dir\n",
        "        self.s_img_dir = semantic_img_dir\n",
        "        self.n_img_dir = nonsemantic_img_dir\n",
        "\n",
        "        self.d_imgs = sorted(os.listdir(self.d_img_dir))\n",
        "        self.s_imgs = sorted(os.listdir(self.s_img_dir))\n",
        "        self.n_imgs = sorted(os.listdir(self.n_img_dir))\n",
        "\n",
        "        self.splits = json.load(open(splits_json, 'r'))\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            self.batch_size = train_batch_size\n",
        "            self.seq_per_img = 1\n",
        "            self.split_idxs = self.splits['train']\n",
        "            self.num_samples = len(self.split_idxs)\n",
        "            if max_samples is not None:\n",
        "                self.num_samples = min(max_samples, self.num_samples)\n",
        "        elif split == 'val': \n",
        "            self.batch_size = val_batch_size\n",
        "            self.seq_per_img = 5\n",
        "            self.split_idxs = self.splits['val']\n",
        "            self.num_samples = len(self.split_idxs)\n",
        "            if max_samples is not None:\n",
        "                self.num_samples = min(max_samples, self.num_samples)\n",
        "        elif split == 'test': \n",
        "            self.batch_size = test_batch_size\n",
        "            self.seq_per_img = 5\n",
        "            self.split_idxs = self.splits['test']\n",
        "            self.num_samples = len(self.split_idxs)\n",
        "            if max_samples is not None:\n",
        "                self.num_samples = min(max_samples, self.num_samples)\n",
        "        else:\n",
        "            raise Exception('Unknown data split %s' % split)\n",
        "\n",
        "        print(\"Dataset size for %s: %d\" % (split, self.num_samples))\n",
        "\n",
        "        # load in the sequence data\n",
        "        self.h5_label_file = h5py.File(h5_label_file, 'r')\n",
        "        seq_size = self.h5_label_file['labels'].shape\n",
        "        self.labels = self.h5_label_file['labels'][:] # just gonna load...\n",
        "        self.neg_labels = self.h5_label_file['neg_labels'][:]\n",
        "        self.max_seq_length = seq_size[1]\n",
        "        self.label_start_idx = self.h5_label_file['label_start_idx'][:]\n",
        "        self.label_end_idx = self.h5_label_file['label_end_idx'][:]\n",
        "        self.neg_label_start_idx = self.h5_label_file['neg_label_start_idx'][:]\n",
        "        self.neg_label_end_idx = self.h5_label_file['neg_label_end_idx'][:]\n",
        "        print('Max sequence length is %d' % self.max_seq_length)\n",
        "        self.h5_label_file.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        random.seed()\n",
        "        img_idx = self.split_idxs[index]\n",
        "\n",
        "        # Fetch image data\n",
        "        # one easy way to augment data is to use nonsemantically changed\n",
        "        # scene as the default :)\n",
        "        if self.split == 'train':\n",
        "            if random.random() < 0.5:\n",
        "                d_feat_path = os.path.join(self.d_feat_dir, self.d_feats[img_idx])\n",
        "                d_img_path = os.path.join(self.d_img_dir, self.d_imgs[img_idx])\n",
        "                n_feat_path = os.path.join(self.n_feat_dir, self.n_feats[img_idx])\n",
        "                n_img_path = os.path.join(self.n_img_dir, self.n_imgs[img_idx])\n",
        "            else:\n",
        "                d_feat_path = os.path.join(self.n_feat_dir, self.n_feats[img_idx])\n",
        "                d_img_path = os.path.join(self.n_img_dir, self.n_imgs[img_idx])\n",
        "                n_feat_path = os.path.join(self.d_feat_dir, self.d_feats[img_idx])\n",
        "                n_img_path = os.path.join(self.d_img_dir, self.d_imgs[img_idx])\n",
        "        else:\n",
        "            d_feat_path = os.path.join(self.d_feat_dir, self.d_feats[img_idx])\n",
        "            d_img_path = os.path.join(self.d_img_dir, self.d_imgs[img_idx])\n",
        "            n_feat_path = os.path.join(self.n_feat_dir, self.n_feats[img_idx])\n",
        "            n_img_path = os.path.join(self.n_img_dir, self.n_imgs[img_idx])\n",
        "\n",
        "\n",
        "        q_feat_path = os.path.join(self.s_feat_dir, self.s_feats[img_idx])\n",
        "        q_img_path = os.path.join(self.s_img_dir, self.s_imgs[img_idx])\n",
        "\n",
        "        d_feature = torch.FloatTensor(np.load(d_feat_path))\n",
        "        n_feature = torch.FloatTensor(np.load(n_feat_path))\n",
        "        q_feature = torch.FloatTensor(np.load(q_feat_path))\n",
        "\n",
        "        # Fetch change type labels\n",
        "        aux_label_pos = -1\n",
        "        for type, img_set in self.type_to_img.items():\n",
        "            if img_idx in img_set:\n",
        "                aux_label_pos = self.type_to_label[type]\n",
        "                break\n",
        "        aux_label_neg = self.type_to_label['no_change']\n",
        "\n",
        "        # Fetch sequence labels\n",
        "        ix1 = self.label_start_idx[img_idx]\n",
        "        ix2 = self.label_end_idx[img_idx]\n",
        "        n_cap = ix2 - ix1 + 1\n",
        "\n",
        "        seq = np.zeros([self.seq_per_img, self.max_seq_length + 1], dtype=int)\n",
        "        if n_cap < self.seq_per_img:\n",
        "            # we need to subsample (with replacement)\n",
        "            for q in range(self.seq_per_img):\n",
        "                ixl = random.randint(ix1, ix2)\n",
        "                seq[q, :self.max_seq_length] = \\\n",
        "                    self.labels[ixl, :self.max_seq_length]\n",
        "        else:\n",
        "            ixl = random.randint(ix1, ix2 - self.seq_per_img + 1)\n",
        "            seq[:, :self.max_seq_length] = \\\n",
        "                self.labels[ixl: ixl + self.seq_per_img, :self.max_seq_length]\n",
        "\n",
        "        # Fetch negative sequence labels\n",
        "        ix1 = self.neg_label_start_idx[img_idx]\n",
        "        ix2 = self.neg_label_end_idx[img_idx]\n",
        "        n_cap = ix2 - ix1 + 1\n",
        "\n",
        "        neg_seq = np.zeros([self.seq_per_img, self.max_seq_length + 1], dtype=int)\n",
        "        if n_cap < self.seq_per_img:\n",
        "            # we need to subsample (with replacement)\n",
        "            for q in range(self.seq_per_img):\n",
        "                ixl = random.randint(ix1, ix2)\n",
        "                neg_seq[q, :self.max_seq_length] = \\\n",
        "                    self.neg_labels[ixl, :self.max_seq_length]\n",
        "        else:\n",
        "            ixl = random.randint(ix1, ix2 - self.seq_per_img + 1)\n",
        "            neg_seq[:, :self.max_seq_length] = \\\n",
        "                self.neg_labels[ixl: ixl + self.seq_per_img, :self.max_seq_length]\n",
        "\n",
        "        # Generate masks\n",
        "        mask = np.zeros_like(seq)\n",
        "        nonzeros = np.array(list(map(lambda x: (x != 0).sum() + 1, seq)))\n",
        "        for ix, row in enumerate(mask):\n",
        "            row[:nonzeros[ix]] = 1\n",
        "\n",
        "        neg_mask = np.zeros_like(neg_seq)\n",
        "        nonzeros = np.array(list(map(lambda x: (x != 0).sum() + 1, neg_seq)))\n",
        "        for ix, row in enumerate(neg_mask):\n",
        "            row[:nonzeros[ix]] = 1\n",
        "\n",
        "        return (d_feature, n_feature, q_feature,\n",
        "                seq, neg_seq, mask, neg_mask, aux_label_pos, aux_label_neg,\n",
        "                d_img_path, n_img_path, q_img_path)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_idx_to_word(self):\n",
        "        return self.idx_to_word\n",
        "\n",
        "    def get_word_to_idx(self):\n",
        "        return self.word_to_idx\n",
        "\n",
        "    def get_max_seq_length(self):\n",
        "        return self.max_seq_length\n",
        "\n",
        "def rcc_collate(batch):\n",
        "    transposed = list(zip(*batch))\n",
        "    d_feat_batch = transposed[0]\n",
        "    n_feat_batch = transposed[1]\n",
        "    q_feat_batch = transposed[2]\n",
        "    seq_batch = default_collate(transposed[3])\n",
        "    neg_seq_batch = default_collate(transposed[4])\n",
        "    mask_batch = default_collate(transposed[5])\n",
        "    neg_mask_batch = default_collate(transposed[6])\n",
        "    aux_label_pos_batch = default_collate(transposed[7])\n",
        "    aux_label_neg_batch = default_collate(transposed[8])\n",
        "    if any(f is not None for f in d_feat_batch):\n",
        "        d_feat_batch = default_collate(d_feat_batch)\n",
        "    if any(f is not None for f in n_feat_batch):\n",
        "        n_feat_batch = default_collate(n_feat_batch)\n",
        "    if any(f is not None for f in q_feat_batch):\n",
        "        q_feat_batch = default_collate(q_feat_batch)\n",
        "\n",
        "    d_img_batch = transposed[9]\n",
        "    n_img_batch = transposed[10]\n",
        "    q_img_batch = transposed[11]\n",
        "    return (d_feat_batch, n_feat_batch, q_feat_batch,\n",
        "            seq_batch, neg_seq_batch,\n",
        "            mask_batch, neg_mask_batch,\n",
        "            aux_label_pos_batch, aux_label_neg_batch,\n",
        "            d_img_batch, n_img_batch, q_img_batch)\n",
        "\n",
        "class RCCDataLoader(DataLoader):\n",
        "    \n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        kwargs['collate_fn'] = rcc_collate\n",
        "        super().__init__(dataset, **kwargs)\n"
      ],
      "metadata": {
        "id": "V1ilrapUEH6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DataLoader"
      ],
      "metadata": {
        "id": "oTK3bpauOiln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset2(split='train'):\n",
        "\n",
        "  dataset = RCCDataset(split)\n",
        "  data_loader = RCCDataLoader(\n",
        "    dataset,\n",
        "    batch_size=dataset.batch_size,\n",
        "    shuffle=True if split == 'train' else False,\n",
        "    num_workers=8)\n",
        "\n",
        "    \n",
        "  return dataset, data_loader"
      ],
      "metadata": {
        "id": "h6RZcGd7Oley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading part\n",
        "#train_dataset, train_loader = create_dataset2('train')\n",
        "#idx_to_word = train_dataset.get_idx_to_word()\n",
        "test_dataset, test_loader = create_dataset2('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-Nm8DD1OfJU",
        "outputId": "5a88b36f-4d9a-4da2-b05b-0b09dc3fee37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker Dataset loading vocab json file:  /content/cripp_trial/jsons/vocab.json\n",
            "vocab size is  76\n",
            "Dataset size for test: 62\n",
            "Max sequence length is 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word = test_dataset.get_idx_to_word()"
      ],
      "metadata": {
        "id": "D2e0KwX9VtrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EVAL LOOP"
      ],
      "metadata": {
        "id": "qiGxFFcLVAPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_mode('eval', [change_detector, speaker])\n",
        "with torch.no_grad():\n",
        "    test_iter_start_time = time.time()\n",
        "\n",
        "    result_sents_pos = {}\n",
        "    result_sents_neg = {}\n",
        "    for i, batch in tqdm(enumerate(test_loader)):\n",
        "\n",
        "        d_feats, nsc_feats, sc_feats, \\\n",
        "        labels, no_chg_labels, masks, no_chg_masks, aux_labels_pos, aux_labels_neg, \\\n",
        "        d_img_paths, nsc_img_paths, sc_img_paths = batch\n",
        "\n",
        "        batch_size = d_feats.size(0)\n",
        "\n",
        "        d_feats, nsc_feats, sc_feats = d_feats.to(device), nsc_feats.to(device), sc_feats.to(device)\n",
        "        d_feats, nsc_feats, sc_feats = \\\n",
        "            spatial_info(d_feats), spatial_info(nsc_feats), spatial_info(sc_feats)\n",
        "        labels, masks = labels.to(device), masks.to(device)\n",
        "        no_chg_labels, no_chg_masks = no_chg_labels.to(device), no_chg_masks.to(device)\n",
        "        aux_labels_pos, aux_labels_neg = aux_labels_pos.to(device), aux_labels_neg.to(device)\n",
        "\n",
        "        chg_pos_logits, chg_pos_att_bef, chg_pos_att_aft, \\\n",
        "        chg_pos_feat_bef, chg_pos_feat_aft, chg_pos_feat_diff = change_detector(d_feats, sc_feats)\n",
        "        chg_neg_logits, chg_neg_att_bef, chg_neg_att_aft, \\\n",
        "        chg_neg_feat_bef, chg_neg_feat_aft, chg_neg_feat_diff = change_detector(d_feats, nsc_feats)\n",
        "\n",
        "\n",
        "        speaker_output_pos, _ = speaker._sample(chg_pos_feat_bef,\n",
        "                                                chg_pos_feat_aft,\n",
        "                                                chg_pos_feat_diff,\n",
        "                                                labels, sample_max=1)\n",
        "\n",
        "        pos_dynamic_atts = speaker.get_module_weights().detach().cpu().numpy()  # (batch, seq_len, 3)\n",
        "\n",
        "        speaker_output_neg, _ = speaker._sample(chg_neg_feat_bef,\n",
        "                                                chg_neg_feat_aft,\n",
        "                                                chg_neg_feat_diff,\n",
        "                                                no_chg_labels, sample_max=1)\n",
        "\n",
        "        neg_dynamic_atts = speaker.get_module_weights().detach().cpu().numpy()  # (batch, seq_len, 3)\n",
        "\n",
        "        gen_sents_pos = decode_sequence(idx_to_word, speaker_output_pos)\n",
        "        gen_sents_neg = decode_sequence(idx_to_word, speaker_output_neg)\n",
        "\n",
        "        chg_pos_att_bef = chg_pos_att_bef.cpu().numpy()\n",
        "        chg_pos_att_aft = chg_pos_att_aft.cpu().numpy()\n",
        "        chg_neg_att_bef = chg_neg_att_bef.cpu().numpy()\n",
        "        chg_neg_att_aft = chg_neg_att_aft.cpu().numpy()\n",
        "        dummy = np.ones_like(chg_pos_att_bef)\n",
        "\n",
        "        for j in range(batch_size):\n",
        "            gts = decode_sequence(idx_to_word, labels[j][:, 1:])\n",
        "            gts_neg = decode_sequence(idx_to_word, no_chg_labels[j][:, 1:])\n",
        "            sent_pos = gen_sents_pos[j]\n",
        "            sent_neg = gen_sents_neg[j]\n",
        "            image_id = d_img_paths[j].split('_')[-1]\n",
        "            result_sents_pos[image_id] = sent_pos\n",
        "            result_sents_neg[image_id + '_n'] = sent_neg\n",
        "            image_num = image_id.split('.')[0]\n",
        "            att_bef_path = os.path.join(att_output_path, image_num + '_before')\n",
        "            att_aft_path = os.path.join(att_output_path, image_num + '_after')\n",
        "            np.save(att_bef_path, chg_pos_att_bef[j])\n",
        "            np.save(att_aft_path, chg_pos_att_aft[j])\n",
        "\n",
        "\n",
        "    test_iter_end_time = time.time() - test_iter_start_time\n",
        "    print('Test took %.4f seconds' % test_iter_end_time)\n",
        "\n",
        "    result_save_path_pos = os.path.join(caption_output_path, 'sc_results.json')\n",
        "    result_save_path_neg = os.path.join(caption_output_path, 'nsc_results.json')\n",
        "    coco_gen_format_save(result_sents_pos, result_save_path_pos)\n",
        "    coco_gen_format_save(result_sents_neg, result_save_path_neg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJMjrZx-R1d8",
        "outputId": "a299bcaa-722d-4d27-f9c6-70959e31d38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "62it [00:16,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test took 16.8172 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}